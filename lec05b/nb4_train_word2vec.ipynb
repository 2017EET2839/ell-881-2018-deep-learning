{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to train word2vec model in Tensorflow\n",
    "\n",
    "* Code adapted from https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_eager.py\n",
    "\n",
    "* Images adapted from http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](w2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import time\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'vocab.txt'\n",
    "src_tgt_file = 'wiki.1M.txt.tokenized.src_tgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us checkout the file we just created! **How many lines does it have**?\n",
    "\n",
    "Now, let us create the dataset, we will use vocab_table to convert word to integer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: Define dataset\n",
    "* Convert word to indexes\n",
    "* Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(src_tgt_file)\n",
    "dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000, count=4, seed=42))\n",
    "dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "dataset = dataset.map(lambda words: (words[0], words[1]))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "dataset = dataset.prefetch(1)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=49, shape=(1024,), dtype=int64, numpy=array([1241,  508, 3151, ...,  329,   33,    8])>,\n",
       " <tf.Tensor: id=50, shape=(1024,), dtype=int64, numpy=array([22641,    57,   508, ...,    33,   145,     0])>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataset_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vocab_table.size()\n",
    "d = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, V, d, num_sampled):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_uniform([V, d]))\n",
    "        self.nce_W = tfe.Variable(tf.truncated_normal([V, d]))\n",
    "        self.nce_b = tfe.Variable(tf.zeros(V))\n",
    "        self.V = V\n",
    "        self.num_sampled = num_sampled\n",
    "        \n",
    "    def compute_loss(self, src_words, tgt_words):\n",
    "        word_vectors = tf.nn.embedding_lookup(self.W, src_words)\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=self.nce_W, biases=self.nce_b, \n",
    "                              labels=tf.expand_dims(tgt_words, axis=1), \n",
    "                              inputs=word_vectors, \n",
    "                              num_sampled=self.num_sampled, num_classes=self.V))\n",
    "        return loss\n",
    "    def call(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = Word2Vec(V, d, num_sampled=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fun = tfe.implicit_value_and_gradients(w2vec.compute_loss)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000 Loss: 30.501628875732422 Time:  38.626\n",
      "Step: 2000 Loss: 27.82444190979004 Time:  38.145\n",
      "Step: 3000 Loss: 26.081783294677734 Time:  37.772\n",
      "Step: 4000 Loss: 26.036054611206055 Time:  37.963\n",
      "Step: 5000 Loss: 24.60239601135254 Time:  33.482\n",
      "Step: 6000 Loss: 24.336790084838867 Time:  26.205\n",
      "Step: 7000 Loss: 24.18866539001465 Time:  25.957\n",
      "Step: 8000 Loss: 23.636003494262695 Time:  26.333\n",
      "Step: 9000 Loss: 23.27335548400879 Time:  26.065\n",
      "Step: 10000 Loss: 22.916406631469727 Time:  26.038\n",
      "Step: 11000 Loss: 21.81801414489746 Time:  25.967\n",
      "Step: 12000 Loss: 22.540122985839844 Time:  25.976\n",
      "Step: 13000 Loss: 22.07505226135254 Time:  26.018\n",
      "Step: 14000 Loss: 21.853620529174805 Time:  25.989\n",
      "Step: 15000 Loss: 21.907264709472656 Time:  26.063\n",
      "Step: 16000 Loss: 21.146833419799805 Time:  25.954\n",
      "Step: 17000 Loss: 22.04213523864746 Time:  26.093\n",
      "Step: 18000 Loss: 21.222883224487305 Time:  25.965\n",
      "Step: 19000 Loss: 20.93439483642578 Time:  26.048\n",
      "Step: 20000 Loss: 20.833341598510742 Time:  25.930\n",
      "Step: 21000 Loss: 20.034090042114258 Time:  26.078\n",
      "Step: 22000 Loss: 20.980321884155273 Time:  25.994\n",
      "Step: 23000 Loss: 20.174226760864258 Time:  26.274\n",
      "Step: 24000 Loss: 20.82882308959961 Time:  26.094\n",
      "Step: 25000 Loss: 19.714279174804688 Time:  26.075\n",
      "Step: 26000 Loss: 20.11513900756836 Time:  26.097\n",
      "Step: 27000 Loss: 20.27594566345215 Time:  26.043\n",
      "Step: 28000 Loss: 19.97296142578125 Time:  26.482\n",
      "Step: 29000 Loss: 19.177515029907227 Time:  26.018\n",
      "Step: 30000 Loss: 19.742965698242188 Time:  26.132\n",
      "Step: 31000 Loss: 18.427696228027344 Time:  25.962\n",
      "Step: 32000 Loss: 19.56790542602539 Time:  26.149\n",
      "Step: 33000 Loss: 19.26434898376465 Time:  25.946\n",
      "Step: 34000 Loss: 18.95215606689453 Time:  26.091\n",
      "Step: 35000 Loss: 18.661657333374023 Time:  25.922\n",
      "Step: 36000 Loss: 18.94532585144043 Time:  26.134\n",
      "Step: 37000 Loss: 18.749849319458008 Time:  25.952\n",
      "Step: 38000 Loss: 18.55238914489746 Time:  26.124\n",
      "Step: 39000 Loss: 18.247892379760742 Time:  25.946\n",
      "Step: 40000 Loss: 18.144424438476562 Time:  26.126\n",
      "Step: 41000 Loss: 18.747346878051758 Time:  26.192\n",
      "Step: 42000 Loss: 18.761884689331055 Time:  26.037\n",
      "Step: 43000 Loss: 18.678916931152344 Time:  25.985\n",
      "Step: 44000 Loss: 17.19847297668457 Time:  26.234\n",
      "Step: 45000 Loss: 17.510616302490234 Time:  26.234\n",
      "Step: 46000 Loss: 17.45857048034668 Time:  25.991\n",
      "Step: 47000 Loss: 17.978910446166992 Time:  26.092\n",
      "Step: 48000 Loss: 17.529273986816406 Time:  25.985\n",
      "Step: 49000 Loss: 17.9821720123291 Time:  26.099\n",
      "Step: 50000 Loss: 17.651992797851562 Time:  26.087\n",
      "Step: 51000 Loss: 17.24679946899414 Time:  26.167\n",
      "Step: 52000 Loss: 17.652009963989258 Time:  26.468\n",
      "Step: 53000 Loss: 17.200214385986328 Time:  28.694\n",
      "Step: 54000 Loss: 17.59037971496582 Time:  13302.296\n",
      "Step: 55000 Loss: 16.63129234313965 Time:  249.534\n",
      "Step: 56000 Loss: 16.951156616210938 Time:  35181.020\n",
      "Step: 57000 Loss: 17.183414459228516 Time:  29.378\n",
      "Step: 58000 Loss: 16.96302032470703 Time:  27.668\n",
      "Step: 59000 Loss: 16.77796173095703 Time:  26.611\n",
      "Step: 60000 Loss: 16.906835556030273 Time:  26.130\n",
      "Step: 61000 Loss: 16.953882217407227 Time:  26.714\n",
      "Step: 62000 Loss: 16.612529754638672 Time:  26.227\n",
      "Step: 63000 Loss: 17.14789390563965 Time:  27.893\n",
      "Step: 64000 Loss: 16.880338668823242 Time:  28.071\n",
      "Step: 65000 Loss: 16.66766357421875 Time:  27.201\n",
      "Step: 66000 Loss: 16.72564125061035 Time:  26.466\n",
      "Step: 67000 Loss: 16.400522232055664 Time:  26.827\n",
      "Step: 68000 Loss: 16.988981246948242 Time:  26.187\n",
      "Step: 69000 Loss: 16.75001335144043 Time:  26.467\n",
      "Step: 70000 Loss: 16.37846565246582 Time:  27.133\n",
      "Step: 71000 Loss: 16.021963119506836 Time:  27.627\n",
      "Step: 72000 Loss: 16.178964614868164 Time:  27.903\n",
      "Step: 73000 Loss: 15.491211891174316 Time:  28.217\n",
      "Step: 74000 Loss: 15.711877822875977 Time:  27.812\n",
      "Step: 75000 Loss: 15.60534381866455 Time:  27.187\n",
      "Step: 76000 Loss: 15.74624252319336 Time:  28.162\n",
      "Step: 77000 Loss: 15.968669891357422 Time:  27.191\n",
      "Step: 78000 Loss: 15.485596656799316 Time:  27.363\n",
      "Step: 79000 Loss: 16.265920639038086 Time:  26.364\n",
      "Step: 80000 Loss: 16.183940887451172 Time:  27.148\n",
      "Step: 81000 Loss: 15.61079216003418 Time:  27.086\n",
      "Step: 82000 Loss: 15.50064754486084 Time:  27.243\n",
      "Step: 83000 Loss: 16.15472984313965 Time:  26.813\n",
      "Step: 84000 Loss: 16.217435836791992 Time:  27.386\n",
      "Step: 85000 Loss: 15.466832160949707 Time:  26.279\n",
      "Step: 86000 Loss: 15.499149322509766 Time:  26.516\n",
      "Step: 87000 Loss: 15.085394859313965 Time:  26.836\n",
      "Step: 88000 Loss: 15.765385627746582 Time:  26.571\n",
      "Step: 89000 Loss: 14.867894172668457 Time:  26.576\n",
      "Step: 90000 Loss: 15.389244079589844 Time:  27.228\n",
      "Step: 91000 Loss: 15.461893081665039 Time:  26.603\n",
      "Step: 92000 Loss: 15.160706520080566 Time:  26.628\n",
      "Step: 93000 Loss: 15.082032203674316 Time:  26.482\n",
      "Step: 94000 Loss: 15.070901870727539 Time:  26.920\n",
      "Step: 95000 Loss: 15.803755760192871 Time:  26.734\n",
      "Step: 96000 Loss: 14.79391860961914 Time:  26.679\n",
      "Step: 97000 Loss: 15.395183563232422 Time:  26.432\n",
      "Step: 98000 Loss: 14.295679092407227 Time:  26.603\n",
      "Step: 99000 Loss: 14.874335289001465 Time:  26.608\n",
      "Step: 100000 Loss: 14.907464027404785 Time:  26.862\n",
      "Step: 101000 Loss: 15.0928955078125 Time:  28.539\n",
      "Step: 102000 Loss: 14.409616470336914 Time:  26.915\n",
      "Step: 103000 Loss: 14.611616134643555 Time:  26.591\n",
      "Step: 104000 Loss: 15.073392868041992 Time:  26.564\n",
      "Step: 105000 Loss: 14.41511344909668 Time:  27.147\n",
      "Step: 106000 Loss: 14.733719825744629 Time:  28.033\n",
      "Step: 107000 Loss: 14.638915061950684 Time:  27.631\n",
      "Step: 108000 Loss: 14.534852981567383 Time:  27.488\n",
      "Step: 109000 Loss: 14.878952026367188 Time:  26.916\n",
      "Step: 110000 Loss: 14.588025093078613 Time:  26.734\n",
      "Step: 111000 Loss: 14.3162260055542 Time:  26.754\n",
      "Final Steps: 111591\n"
     ]
    }
   ],
   "source": [
    "train_step = 0\n",
    "total_loss = 0.\n",
    "STATS_STEP = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for src_words, tgt_words in dataset:\n",
    "    loss_batch, gradients = grad_fun(src_words, tgt_words)\n",
    "    total_loss += loss_batch\n",
    "    opt.apply_gradients(gradients)\n",
    "    train_step += 1\n",
    "    if train_step % STATS_STEP == 0:\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f'Step: {train_step} Loss: {total_loss/STATS_STEP} Time: {time_taken: 0.3f}')\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "print(f'Final Steps: {train_step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_prefix = os.path.join('ckpt', 'w2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tfe.Checkpoint(model=w2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.checkpointable.util.Checkpoint object at 0x12c86ff28>\n"
     ]
    }
   ],
   "source": [
    "print(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ckpt.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt/w2vec-1\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
