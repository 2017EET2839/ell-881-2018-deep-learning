{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: Get Data\n",
    "\n",
    "* We will work with (**a part of**) English Wikipedia. \n",
    "* English Wikipedia has **more than 55M** sentences! \n",
    "* For today's class, we will work with 1M sentences.\n",
    "* If you are interested in how to extract sentences from English Wikipedia, you can look at this [code](https://github.com/vineetm/tf-similar-sentences/blob/master/code/extract_sentences.py), which uses gensim Wikicorpus to extract data from raw Dump of 15G!\n",
    "\n",
    "Let us begin by extracting the zip with 1M sentenes. \n",
    "\n",
    "Run the following command in your terminal/shell:\n",
    "```bash\n",
    "cd ../data\n",
    "unzip wiki.1M.txt.zip\n",
    "cd -\n",
    "```\n",
    "\n",
    "\n",
    "### P2: Explore the data\n",
    "The first thing that you should **always** do when you start to work with *any* data, is to explore it! \n",
    "\n",
    "* Let us count the lines in the file:\n",
    "```bash\n",
    "wc -l ../data/wiki.1M.txt\n",
    "```\n",
    "\n",
    "* Let us now look at some lines from the file\n",
    "```\n",
    "head ../data/wiki.1M.txt\n",
    "```\n",
    "\n",
    "### P3: Tokenization\n",
    "Tokenization is a process to convert a word such as *Delhi's* into its constituent parts(generally words): *Delhi* and *'s*\n",
    "\n",
    "**QS**: Why do we care if **Delhi's** is split into two tokens?\n",
    "\n",
    "**Note**: We demonstrate how to tokenize using [nltk](https://www.nltk.org/) here. You can use other alternatives such as [spacy](https://spacy.io/)\n",
    "\n",
    "If the following does not work install nltk data\n",
    "```bash\n",
    "python -m nltk.downloader 'punkt'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us define some handles for input, output and vocab files\n",
    "input_file = '../data/wiki.1M.txt'\n",
    "output_file = '../data/wiki.1M.txt.tokenized'\n",
    "vocab_file = '../data/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Delhi', \"'s\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Delhi's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4: Tokenize all sentences in a file\n",
    "\n",
    "Now that we know how to tokenize a single sentence, let us create a **tokenized** version of `wiki.1M.txt`. \n",
    "\n",
    "More concretely:\n",
    "1. Lowercase the sentence\n",
    "2. Tokenize the sentence\n",
    "3. Join tokens with space ' ' and write the **tokenized** sentence to file...\n",
    "4. Print progress, say every 100K sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fill in the method below:\n",
    "def convert_to_tokens(input_file, output_file):\n",
    "    with open(input_file) as fr,open(output_file, 'w') as fw:\n",
    "        for index, sentence in enumerate(fr):\n",
    "            words = word_tokenize(sentence.strip().lower())\n",
    "            fw.write(f\"{' '.join(words)}\\n\")\n",
    "            if index % 100000 == 0:\n",
    "                print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Time Taken: 109.22984576225281s\n"
     ]
    }
   ],
   "source": [
    "#This should take about 2 minutes. Think that is long! Think again, you processed 1M sentences in under 2 minutes!\n",
    "start = time.time()\n",
    "convert_to_tokens(input_file, output_file)\n",
    "print(f'Time Taken: {time.time()-start}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5: Peek at the new file\n",
    "Now, let us take a look at our newly created file:\n",
    "1. Check if it has same number of lines as the raw file\n",
    "```bash\n",
    "wc -l ../data/wiki.1M.txt.tokenized\n",
    "```\n",
    "\n",
    "2. Check a few lines and compare it with the raw file:\n",
    "```bash\n",
    "head ../data/wiki.1M.txt\n",
    "head ../data/wiki.1M.txt.tokenized\n",
    "```\n",
    "\n",
    "### P6: Building Vocabulary\n",
    "* Vocabulary refers to what words appear in your corpus\n",
    "* To make learning tractable, we **restrict our vocabulary** \n",
    "* You can restrict using any measure, but as we are dealing with data-driven methods, its intuitive to restrict using **word counts**\n",
    "\n",
    "\n",
    "* All words which appear with a **minimum frequency** are retained, and rest all words are assigned special unknown symbol **UNK**\n",
    "* Another popular method is to take Top-$K$ words.\n",
    "\n",
    "Now, we will use our freshly minted `wiki.1M.txt.tokenized` to count words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Count words and return words in a sorted order\n",
    "def count_words(sentences_file):\n",
    "    counter = dict()\n",
    "    for sentence in open(sentences_file):\n",
    "        words = sentence.strip().split()\n",
    "        for word in words:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "    return sorted(counter.items(), key=lambda pair:pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 929180),\n",
       " (',', 822110),\n",
       " ('.', 580671),\n",
       " ('of', 477522),\n",
       " ('and', 376517),\n",
       " ('in', 337457),\n",
       " ('to', 280731),\n",
       " ('a', 259190),\n",
       " (')', 137015),\n",
       " ('(', 135461)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = count_words(output_file)\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7: Assign word index\n",
    "\n",
    "1. Next, we want to create a map, which returns an **integer** for a word\n",
    "2. As we have restricted our vocabulary, we want to return index 0 for words we removed\n",
    "3. These words are also known as Out of Vocabulary (OOV) words\n",
    "4. Assign a unique integer to words in vocabulary, starting from 1\n",
    "5. For consistency, we will assign lower indexe to a higher frequency word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a word->integer mapping\n",
    "# Discard words which have count less than min freq\n",
    "# Assign index 0 to `UNK`\n",
    "def build_vocab(word_counts, min_freq):\n",
    "    vocab = OrderedDict()\n",
    "    vocab['UNK'] = 0\n",
    "    \n",
    "    for word, freq in word_counts:\n",
    "        if freq < min_freq:\n",
    "            return vocab\n",
    "        vocab[word] = len(vocab) + 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V[UNK]: 0 V[\"learning\"]: 1552\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(word_counts, 10)\n",
    "print(f'V[UNK]: {vocab[\"UNK\"]} V[\"learning\"]: {vocab[\"learning\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a list of word indexes for all words in sentence\n",
    "def assign_word_indexes(sentence, vocab):\n",
    "    return [vocab[word] if word in vocab else vocab['UNK'] for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = assign_word_indexes('i am learning to build vocab', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117, 1790, 1552, 8, 1785, 0]\n"
     ]
    }
   ],
   "source": [
    "print(word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46014"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8: Write Vocabulary file\n",
    "* Write each word in vocabulary to a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(vocab_file, vocab):\n",
    "    with open(vocab_file, 'w') as fw:\n",
    "        for word in vocab:\n",
    "            fw.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(vocab_file, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines in our vocab file?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
