{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn concretely how to build a neural language model (LM)\n",
    "\n",
    "The code has been adapted from the [official tutorial on using eager for LM](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/rnn_ptb/rnn_ptb.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set a **fixed random seed**! So you can reproduce the experiments!\n",
    "\n",
    "For notebook, this only works when you restart the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: Embedding Model\n",
    "Let us begin, by building an **Embedding Model**. The job of embedding model is simple: Given a tensor of word indexes, return corresponding vectors (or rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, V, d):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "    \n",
    "    def call(self, word_indexes):\n",
    "        return tf.nn.embedding_lookup(self.W, word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give it a try by finding embeddings for word indexes: 5 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = Embedding(5000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = word_embeddings([5, 100])\n",
    "print(vecs.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = word_embeddings([[5, 100, 40], [2, 300, 90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecs.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: RNN Cell...\n",
    "\n",
    "Now, we will learn how an RNN Cell operates. \n",
    "\n",
    "Recall, how RNN updates its hidden state at time step $t$\n",
    "\n",
    "$h_t = f(x_{t}, h_{t-1})$\n",
    "\n",
    "\n",
    "**Question**: What should be the initial state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume we have a batch of 2 sentences, each sentence has 3 words. \n",
    "\n",
    "We will come to how RNN will handle variable length sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = [[20, 30, 400], [500, 0, 3]]\n",
    "word_vectors = word_embeddings(word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What should be shape of word_vectors? Recall em returns vectors of size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we will not be able to pass the word_vectors directly. RNN proceses inputs **one time step** at a time!\n",
    "\n",
    "Enter, [tf.unstack](https://www.tensorflow.org/api_docs/python/tf/unstack)\n",
    "![title](tf.unstack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "print(f'word_vectors_time: len:{len(word_vectors_time)} Shape[0]: {word_vectors_time[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.BasicRNNCell(256)\n",
    "init_state = cell.zero_state(batch_size=int(word_vectors.shape[0]), dtype=tf.float32)\n",
    "output, state = cell(word_vectors_time[0], init_state)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You might be wondering: We only talked about hidden state $h_t$ till now, why do we have two vectors being computed output and state. \n",
    "\n",
    "* For a BasicRNNCell output and state are identical. \n",
    "\n",
    "* For LSTM and GRU they have different meaning. All we need to understand is that it uses state and output to do its magic of being able to maintain and learn long term dependencies. \n",
    "\n",
    "* We would mostly use state to pass it to next time step, and output to make predictions at that time step.\n",
    "\n",
    "* Read this [excellent blog post on LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), in case you are interested in how LSTM works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3: RNN Model\n",
    "Now, we have all the pieces to build an RNN Model. Let us see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(RNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, word_vectors):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs = []\n",
    "        \n",
    "        state = self.cell.zero_state(batch_size=int(word_vectors.shape[0]), dtype=tf.float32)\n",
    "        for word_vector_time in word_vectors_time:\n",
    "            output, state = self.cell(word_vector_time, state)\n",
    "            outputs.append(output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = [[20, 30, 400], [500, 0, 3]]\n",
    "word_vectors = word_embeddings(word_indexes)\n",
    "\n",
    "rnn = RNN(128, 'rnn')\n",
    "rnn_outputs = rnn(word_vectors)\n",
    "\n",
    "# Prints \"Num outputs: 3 Shape[0]: (2, 128)\"\n",
    "print(f'Num outputs: {len(rnn_outputs)} Shape[0]: {rnn_outputs[0].numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4: Data pipeline\n",
    "\n",
    "We will work with a standard LM dataset: PTB dataset from Tomas Mikolov's webpage:\n",
    "```bash\n",
    "wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "tar xvf simple-examples.tgz\n",
    "```\n",
    "\n",
    "The first thing, we do with any data is to take a peek at it. \n",
    "\n",
    "```bash\n",
    "head -3 simple-examples/data/ptb.train.txt\n",
    "```\n",
    "\n",
    "```\n",
    "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
    "pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
    "mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
    "```\n",
    "Some key points to note:\n",
    "\n",
    "* We see here that there is a $<unk>$ token already.\n",
    "* There also seems another token $N$. This identifies a number. \n",
    "* Rest all words seem to be lower cased\n",
    "    \n",
    "Let us count up the vocab quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'simple-examples/data/ptb.train.txt'\n",
    "UNK='<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentences_file):\n",
    "    counter = {}\n",
    "    for sentence in open(sentences_file):\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = count_words(train_file)\n",
    "print(f'Num unique words: {len(counter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a special token EOS which signifies end of sentence. We add this to out vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write the vocab to file. Since, we are using OrderedDict, we will get words in order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(counter, vocab_file, unk=UNK, eos=EOS):\n",
    "    del counter[unk]\n",
    "    with open(vocab_file, 'w') as fw:\n",
    "        fw.write(f'{unk}\\n')\n",
    "        fw.write(f'{eos}\\n')\n",
    "        for word, _ in sorted(counter.items(), key=lambda pair:pair[1], reverse=True):\n",
    "            fw.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'simple-examples/data/vocab.txt'\n",
    "write_vocab(counter, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peek at vocab file, see if the words make sense...\n",
    "\n",
    "```bash\n",
    " head simple-examples/data/vocab.txt \n",
    "```\n",
    "\n",
    "This generates the following:\n",
    "```\n",
    "<unk>\n",
    "<eos>\n",
    "the\n",
    "N\n",
    "of\n",
    "to\n",
    "a\n",
    "in\n",
    "and\n",
    "'s\n",
    "```\n",
    "\n",
    "Next, we want to create a data pipeline, we would create a batch of src words and corresponding target words.\n",
    "\n",
    "Target words would be shifted right by one. Let us give a concrete example:\n",
    "\n",
    "**Sentence**: \"the cat sat on mat\"\n",
    "\n",
    "**Src_Words:**: ['the', 'cat', 'sat', 'on', 'mat']\n",
    "\n",
    "**Tgt_Words:**: ['cat', 'sat', 'on', 'mat', '<eos\\>']\n",
    "\n",
    "<img src=\"data_tx@2x.png\" alt=\"drawing\" width=\"300\"/>\n",
    "Let us begin by creating a vocab table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file(vocab_file)\n",
    "vocab_table.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"cross_entropy@2x.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sentences_file, vocab_table, batch_size, eos=EOS):\n",
    "    #Create a Text Line dataset, which returns a string tensor\n",
    "    dataset = tf.data.TextLineDataset(sentences_file)\n",
    "    \n",
    "    #Convert to a list of words..\n",
    "    dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "    \n",
    "    #Create target words right shifted by one, append EOS, also return size of each sentence...\n",
    "    dataset = dataset.map(lambda words: (words, tf.concat([words[1:], [eos]], axis=0), tf.size(words)))\n",
    "    \n",
    "    #Lookup words, word->integer, EOS->1\n",
    "    dataset = dataset.map(lambda src_words, tgt_words, num_words: (vocab_table.lookup(src_words), vocab_table.lookup(tgt_words), num_words))\n",
    "    \n",
    "    #[None] -> src words, [None] -> tgt_words, [] length of sentence\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=([None], [None], []))\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(train_file, vocab_table, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out sample data!\n",
    "\n",
    "next(iter(dataset))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5: RNN Model (revisited)\n",
    "\n",
    "Now, that we have a way to load up data. Let us see how our RNN model behaves.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = Embedding(V=vocab_table.size(), d=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word_embeddings(datum[0])\n",
    "word_vectors.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(h=128, cell='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_outputs = rnn(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num outputs: {len(rnn_outputs)} Shape[0]: {rnn_outputs[0].numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem, with our current RNN implementation is that it processes even past the sentence length. For example, length of sentence 0 is 24, but since longest sentence in first batch is of length 48. It returns outputs even past length 24. Let us confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_outputs[40][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticRNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(StaticRNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, word_vectors, num_words):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        outputs, final_state = tf.nn.static_rnn(cell=self.cell, inputs=word_vectors_time, sequence_length=num_words, dtype=tf.float32)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srnn = StaticRNN(h=128, cell='rnn')\n",
    "rnn_outputs = srnn(word_vectors, datum[2])\n",
    "rnn_outputs[40][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P6: Language Model (Code)\n",
    "\n",
    "At each time step, we want to predict a probability distribution over the entire vocabulary\n",
    "\n",
    "Thus, we need to add an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.word_embedding = Embedding(V, d)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=V)\n",
    "        \n",
    "    def call(self, datum):\n",
    "        word_vectors = self.word_embedding(datum[0])\n",
    "        rnn_outputs_time = self.rnn(word_vectors, datum[2])\n",
    "        \n",
    "        #We want to convert it back to shape batch_size x TimeSteps x h\n",
    "        rnn_outputs = tf.stack(rnn_outputs_time, axis=1)\n",
    "        logits = self.output_layer(rnn_outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(vocab_table.size(), 128, 128, 'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm(datum)\n",
    "print(f'logits shape {logits.numpy().shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7: Loss function\n",
    "\n",
    "* At each time step, RNN makes a prediction\n",
    "* More concretely it generated 10,000 (V) logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute loss by comparing the predictions against true labels. We will use Cross Entropy Loss.\n",
    "\n",
    "* Cross Entropy measures distance between two probability distributions $p$ and $q$.\n",
    "\n",
    "* When you have only one class as correct in true distribution. The Cross entropy simplifies to computing the loss of the target word!\n",
    "\n",
    "<img src=\"cross_entropy@2x.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "* You should never compute the target probability directly. Further as we have our labels with only correct index we would use sparse_softmax_cross_entropy_with_logits. We pass the logits to this method directly!\n",
    "\n",
    "Now let us get some intuition about the loss values..\n",
    "\n",
    "First let us compute cross entropy loss for a model that predicts each word equally likely. In this case the probability would be 1/V or 1/10000. This comes out to be 9.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-tf.log(1/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see what is the loss for the first prediction on an untrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm(datum)\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are not doing any better than making a random prediction! Which is fine as we have not trained our model!\n",
    "\n",
    "\n",
    "Next, we need to be careful about not adding any loss for the **padded values**.\n",
    "\n",
    "Let us check out length of first sentence, and see what are loss values past the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Len of first sentence:  {datum[2][0]} Loss[{datum[2][0]}:]={loss[0][datum[2][0]:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually don't want to accumulate this loss! We will zero it out using sequence mask. Which creates a tensor of 0's and 1's as per the sequence length...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "loss = loss * mask\n",
    "print(f'Len of first sentence:  {datum[2][0]} Loss[{datum[2][0]}:]={loss[0][datum[2][0]:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we are training we would do it over a batch. In this case 32 sentences with many words in each sentence... Thus, we will compute an average loss over this batch\n",
    "\n",
    "We compute this by dividing total loss for the batch by total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "loss = loss * mask\n",
    "avg_loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "print(f'Avg loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(model, datum):\n",
    "    logits = model(datum)\n",
    "    mask = tf.sequence_mask(datum[2], dtype=tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[1]) * mask\n",
    "    return tf.reduce_sum(loss) / tf.cast(tf.reduce_sum(datum[2]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8: Gradients Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grads_fun = tfe.implicit_value_and_gradients(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, gradients_value = loss_and_grads_fun(lm, datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P9: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "STATS_STEPS = 10\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    for step_num, datum in enumerate(dataset, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun(lm, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        opt.apply_gradients(gradients)\n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the loss changed for the first batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_grads_fun(lm, datum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-9.19) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
